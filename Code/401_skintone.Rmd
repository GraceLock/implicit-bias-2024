---
title: "DA401 Project"
author: "Grace Lock"
date: "2024-01-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(haven) #Reading in the data
library(dplyr)
library(tidyverse)
library(socsci) #Helps with data manipulation
library(glmnet) #Lasso Regression
library(reshape2) #Lasso Regression visual 
library(nnet) #Multinomial Logistic Regression
library(rpart) #Regression Tree
library(leaps) #Best subset regression
library(MASS) #Multiordinal logistic regression - messes with dplyr select function
```


### Skin tone data 

```{r}
skintonedata <- read_sav("/Users/gracelock/Downloads/Skin IAT.public.2023 2.sav")

skintonedata |>  dplyr::select("session_id", "birthyear", "birthSex", "politicalid_7",
                                 "num_002", "ethnicityomb", "raceomb_002", "edu", "D_biep.LightSkin_Good_all", 
                                 "att7", "Tdark", "Tlight") |>
                 na.omit("D_biep.LightSkin_Good_all") |>
                 rename("num_tests" = "num_002",
                        "ethnicity" = "ethnicityomb",
                        "race" = "raceomb_002",
                        "politicalid" = "politicalid_7",
                        "skintone_preference" = "att7",
                        "warmth_dark" = "Tdark",
                        "warmth_light" = "Tlight",
                        "score" = "D_biep.LightSkin_Good_all") |>
                 mutate(age = 2024 - birthyear) |>
                 dplyr::select(-birthyear) -> skintonedata

#birthsex: 1 = male, 2 = female

#warmth: 0 "Extremely cold" 1 "Very cold" 2 "Moderately cold" 3 "Somewhat cold" 4 "Slightly cold" 5 "Neither warm nor cold" 6 "Slightly warm" 7 "Somewhat warm" 8 "Moderately warm" 9 "Very warm" 10 "Extremely warm" 

#skintone_preference: 1 "I strongly prefer Dark Skinned People to Light Skinned People." 2 "I moderately prefer Dark Skinned People to Light Skinned People." 3 "I slightly prefer Dark Skinned People to Light Skinned People." 4 "I like Light Skinned People and Dark Skinned People equally." 5 "I slightly prefer Light Skinned People to Dark Skinned People." 6 "I moderately prefer Light Skinned People to Dark Skinned People." 7 "I strongly prefer Light Skinned People to Dark Skinned People." 

#race: 1 = American Indian/Alaska Native, 2	= East Asian, 3 = South Asian, 4 = Native Hawaiian or other Pacific #Islander, 5 = Black or African American, 6	= White, 7 = Other or Unknown, 8 = Multiracial

#ethnicity:1 "Hispanic or Latino" 2 "Not Hispanic or Latino" 3 "Unknown"

#politicalid: 1 "strongly conservative" 2 "moderately conservative" 3 "slightly conservative" 4 "neutral" 5 "slightly liberal" 6 "moderately liberal" 7 "strongly liberal"
```


```{r}
#LASSO Regression 

predictor_vars <- c("num_tests", "ethnicity", "race", "politicalid", "skintone_preference", "warmth_dark",
                "warmth_light", "age", "edu", "birthSex")
response_var <- as.vector(skintonedata$score)

X <- as.matrix(skintonedata[, predictor_vars])  # Predictor matrix
Y <- response_var  # Response variable

# Perform Lasso regression
lasso_model <- glmnet(X, Y, alpha = 1)  # alpha = 1 for Lasso regression

# Plot the cross-validated mean squared error (CV MSE) vs lambda
plot(lasso_model)

# Select lambda with minimum CV MSE
best_lambda <- cv.glmnet(X, Y, alpha = 1)$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste("Mean Squared Error (MSE):", mse))

# Print the coefficients
print(coef(lasso_model_best))
```

Non-zero coefficients: Predictors with non-zero coefficients in the printed output are selected by the Lasso regression model. These predictors are deemed to have a significant influence on the response variable.

Zero coefficients: Predictors with coefficients set to zero in the printed output are effectively excluded from the model. These predictors are considered to have little or no impact on the response variable according to the Lasso regularization.

Coefficient values: The magnitude of the coefficients indicates the strength of the relationship between each predictor and the response variable. Larger coefficient values suggest stronger relationships, whereas smaller values suggest weaker relationships.

Intercept term: Lasso regression also estimates an intercept term, which represents the expected value of the response variable when all predictor variables are zero.

```{r}
#LASSO Regression 2
predictor_vars <- c('num_tests', 'ethnicity', 'race', 'politicalid', 'skintone_preference', 'warmth_dark',
                'warmth_light', 'age', 'edu', 'birthSex')

X <- as.matrix(skintonedata[, predictor_vars])  # Predictor matrix
Y <- skintonedata$score  # Response variable

# Create a grid of lambda values for cross-validation
lambda_grid <- 10^seq(10, -2, length = 100)

# Perform cross-validated Lasso regression
lasso_model_cv <- cv.glmnet(X, Y, alpha = 1, lambda = lambda_grid, nfolds = 10)

# Plot mean squared error (MSE) vs lambda
plot(lasso_model_cv)

# Select lambda with minimum MSE
best_lambda <- lasso_model_cv$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste('Mean Squared Error (MSE):', mse))

# Print the coefficients
print(coef(lasso_model_best))

print(best_lambda)

```


```{r}
#Dot Plot 
df <- data.frame(
  Row = c("Sex", "Education", "Political Identity", "Age", "Race", "Ethnicity", "Number of Tests", 
                  "Warmth 1", "Warmth 2", "Warmth 3"),
  Skintone = c(1, 1, 1, 1, 0, 1, 1, 1, 1, 0),
  Weight = c(1, 0, 1, 1, 0, 0, 1, 1, 1, 1),
  Gender = c(1, 0, 1, 1, 0, 0, 1, 1, 1, 0),
  Sexuality = c(1, 0, 1, 1, 1, 0, 1, 1, 1, 1)
)

data_long <- melt(df, id.vars = "Row", variable.name = "Category", value.name = "Value")
data_long$Row <- factor(data_long$Row, levels = c("Political Identity", "Number of Tests", "Age", "Warmth 3",
                                                  "Warmth 2", "Warmth 1", "Sex", "Race", "Ethnicity", "Education"))

data_long |> 
  ggplot() + 
  aes(x = Category, y = Row, color = as.factor(Value)) + 
  geom_point(size = 5) +
  scale_color_manual(values = c("red", "green"), labels = c("No", "Yes")) +
  labs(title = "Variable Selection",
       x = "Bias Type",
       y = "Predictors",
       color = "Variable was selected:",
       subtitle = "(variables in the ordinal logistic regression model with the lowest AIC)",
       caption = "Warmth 1, 3, and 3 represent how warm or cold a participant feels towards the groups\nof people in that bias test. See appendix for more specifics on the warmth questions for each test.") +
  theme_minimal() +
  theme(legend.position = "bottom") 
```

```{r}
#Best Subset Selection

# Generate all possible models
  all_models <- regsubsets(score ~ num_tests + ethnicity + race + politicalid + skintone_preference +
                             warmth_dark + warmth_light + age + edu + birthSex, 
                           data = skintonedata, nvmax = 10)
  
  # Get the summary of all models
  summary_all <- summary(all_models)
  
  # Find the best model based on adjusted R^2
  best_model <- which.max(summary_all$adjr2)
  
  # Get the details of the best model
  best_summary <- summary_all[best_model]
  
  # Get the formula of the best model
  formula_best <- names(which(summary_all$which[best_model, ]))
  
  # Print the results
  cat("Best model formula:", paste("y ~", paste(formula_best, collapse = " + ")), "\n")
```

```{r}
#Best Subset Selection Visual 

selections_df <- data.frame(
  nvmax = c(8, 6, 4, 3),
  num_tests = c(1, 1, 1, 1),
  age = c(1, 1, 1, 1),
  skintone_preference = c(1, 1, 1, 1),
  politicalid = c(1, 1, 1, 0),
  ethnicity = c(1, 1, 0, 0),
  edu = c(1, 1, 0, 0),
  warmth_light = c(1, 0, 0, 0),
  birthSex = c(1, 0, 0, 0),
  race = c(0, 0, 0, 0),
  warmth_dark = c(0, 0, 0, 0)
)

data_longer <- melt(selections_df, id.vars = "nvmax", variable.name = "Category", value.name = "Value")
#data_long$Row <- factor(data_long$Row, levels = c("Political Identity", "Number of Tests", "Age", "Warmth 3",
#                                                 "Warmth 2", "Warmth 1", "Sex", "Race", "Ethnicity", "Education"))

data_longer |> 
  ggplot() + 
  aes(x = Category, y = nvmax, color = as.factor(Value)) + 
  geom_point(size = 5) +
  scale_color_manual(values = c("red", "green"), labels = c("No", "Yes")) +
  labs(title = "Best Subset Selection: Skintone Data",
       x = "Predictors",
       y = "Number of predictors to select",
       color = "Variable was selected:") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(
    axis.text.x = element_text(angle = 25, hjust = 1)
  )
```


```{r}
#Create categories

#create categories in score variable (no bias, moderate bias, strong bias)

# Define the breaks for creating three categories
breaks <- c(-Inf, -0.0001, 0.0001, 0.33, 0.66, Inf)

# Create a new categorical variable based on the breaks
skintonedata$scorecat <- cut(skintonedata$score, breaks = breaks, labels = c("Opposite", "None", "Low", "Medium", "High"))

# Print the summary of the new categorical variable
summary(skintonedata$scorecat)
```

```{r}
#Multi ordinal logistic regression 1

# Fit ordinal logistic regression model
ord_model1 <- polr(scorecat ~ num_tests + ethnicity + politicalid + skintone_preference +
                    warmth_light + age + edu + birthSex, data = skintonedata, Hess = TRUE)

# Summarize the model
summary(ord_model1)
```

```{r}
#Multi ordinal logistic regression 2

# Fit ordinal logistic regression model
ord_model2 <- polr(scorecat ~ num_tests + ethnicity + politicalid + skintone_preference +
                     age + edu, 
                   data = skintonedata, Hess = TRUE)

# Summarize the model
summary(ord_model2)
```

Ord model 1 has the lowest AIC. 

```{r}
#table 
ctable <- coef(summary(ord_model1))

#calculate and store p values
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2

## combined table
ctable <- cbind(ctable, "p value" = p)
```

```{r}
#Confidence intervals 
ci <- confint(ord_model1)
ci
```

The 95% confidence intervals do not cross 0 so therefore they are significant - though they are very close to 0. 

```{r}
#Odds Ratio and confidence intervals 
exp(cbind(OR = coef(ord_model1), ci))
```

For every one unit increase in number of tests taken, the odds of being more bias is multiplied .85 times, holding constant all other variables. 
For every one unit increase in ethnicity, the odds of being more bias is multiplied 1.12 times or increases 12%, holding constant all other variables. 
The odds ratio is greatest for skintone_preference showing that this has the greatest impact which is in line with other models. 

```{r}
#Training and Test Data 

# Set seed for reproducibility (allows for the same random split when rerunning)
set.seed(123)

# Generate indices for train and test data
train_indices <- sample(1:nrow(skintonedata), 0.8 * nrow(skintonedata))  # 80% for training
test_indices <- setdiff(1:nrow(skintonedata), train_indices)  # Remaining for testing

# Create training and test data frames
train_data <- skintonedata[train_indices, ]
test_data <- skintonedata[test_indices, ]
```

```{r}
#Ordinal regression predictions 

# Predict classes
predicted_classes <- predict(ord_model2, newdata = test_data, type = "class")

# Create confusion matrix
conf_matrix <- table(test_data$scorecat, predicted_classes)

# Print confusion matrix
print(conf_matrix)
```

### Citations

```{r}
rpart_cit <- citation("rpart")
glmnet_cit <- citation("glmnet")
```


