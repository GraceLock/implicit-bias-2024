---
title: "DA401 Project"
author: "Grace Lock"
date: "2024-01-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(haven) #Reading in the data
library(dplyr)
library(tidyverse)
library(socsci) #Helps with data manipulation
library(glmnet) #Lasso Regression
library(nnet) #Multinomial Logistic Regression
library(pROC) #ROC Visualization (logistic regression)
library(rpart) #Regression Tree
library(leaps) #Best subset regression
```


### Skin tone data 

```{r}
skintonedata <- read_sav("/Users/gracelock/Downloads/Skin IAT.public.2023 2.sav")

skintonedata |>  select("session_id", "birthyear", "birthSex", "politicalid_7",
                                 "num_002", "ethnicityomb", "raceomb_002", "edu", "D_biep.LightSkin_Good_all", 
                                 "att7", "Tdark", "Tlight") |>
                 na.omit("D_biep.LightSkin_Good_all") |>
                 rename("num_tests" = "num_002",
                        "ethnicity" = "ethnicityomb",
                        "race" = "raceomb_002",
                        "politicalid" = "politicalid_7",
                        "skintone_preference" = "att7",
                        "warmth_dark" = "Tdark",
                        "warmth_light" = "Tlight",
                        "score" = "D_biep.LightSkin_Good_all") |>
                 mutate(age = 2024 - birthyear) |>
                 select(-birthyear) -> skintonedata

#birthsex: 1 = male, 2 = female

#warmth: 0 "Extremely cold" 1 "Very cold" 2 "Moderately cold" 3 "Somewhat cold" 4 "Slightly cold" 5 "Neither warm nor cold" 6 "Slightly warm" 7 "Somewhat warm" 8 "Moderately warm" 9 "Very warm" 10 "Extremely warm" 

#skintone_preference: 1 "I strongly prefer Dark Skinned People to Light Skinned People." 2 "I moderately prefer Dark Skinned People to Light Skinned People." 3 "I slightly prefer Dark Skinned People to Light Skinned People." 4 "I like Light Skinned People and Dark Skinned People equally." 5 "I slightly prefer Light Skinned People to Dark Skinned People." 6 "I moderately prefer Light Skinned People to Dark Skinned People." 7 "I strongly prefer Light Skinned People to Dark Skinned People." 

#race: 1 = American Indian/Alaska Native, 2	= East Asian, 3 = South Asian, 4 = Native Hawaiian or other Pacific #Islander, 5 = Black or African American, 6	= White, 7 = Other or Unknown, 8 = Multiracial

#ethnicity:1 "Hispanic or Latino" 2 "Not Hispanic or Latino" 3 "Unknown"

#politicalid: 1 "strongly conservative" 2 "moderately conservative" 3 "slightly conservative" 4 "neutral" 5 "slightly liberal" 6 "moderately liberal" 7 "strongly liberal"
```

```{r}
#Population data 

#Age
skintonedata |> 
  ggplot() + 
  aes(
    x = age
  ) + 
  geom_histogram()

#Race
skintonedata |> 
  mutate(
    race = case_when(
      race %in% c(2, 3) ~ "Asian",
      race == 5 ~ "Black or African American",
      race == 6 ~ "White",
      race %in% c(1, 4, 7) ~ "Other",
      race == 8 ~ "Multiracial",
    ) ) |>
  ggplot() + 
  aes(
    x = race
  ) + 
  geom_bar() 

#Number of tests
skintonedata |> 
  ggplot() + 
  aes(
    x = num_tests
  ) + 
  geom_histogram()

#Political Identification
skintonedata |> 
  mutate(
    politicalid = frcode(
      politicalid == 1 ~ "strongly\nconservative",
      politicalid == 2 ~ "moderately\nconservative",
      politicalid == 3 ~ "slightly\nconservative",
      politicalid == 4 ~ "neutral",
      politicalid == 5 ~ "slightly\nliberal",
      politicalid == 6 ~ "moderately\nliberal",
      politicalid == 7 ~ "strongly\nliberal"
    )
  ) |>
  ggplot() + 
  aes(
    x = politicalid
  ) + 
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Education
skintonedata |>
  mutate(
    edu = frcode(
      edu %in% c(1, 2, 3) ~ "less than high school",
      edu == 4 ~ "high school graduate",
      edu == 5 ~ "some college",
      edu == 6 ~ "associate's degree",
      edu == 7 ~ "bachelor's degree",
      edu %in% c(8, 9, 10, 11, 12, 13, 14) ~ "higher education"
    )
  ) |>
  ggplot() + 
  aes(
    x = edu
  ) + 
  geom_bar()  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Sex
skintonedata |> 
  mutate(
    birthSex = case_when(
      birthSex == 1 ~ "Male",
      birthSex == 2 ~ "Female"
    )
  ) |>
  ggplot() +
  aes(
    x = birthSex
  ) + 
  geom_bar()

#Skin tone Preference
skintonedata |> 
  mutate(
    skintone_preference = frcode(
      skintone_preference == 1 ~ "strongly prefer\nDark Skinned People",
      skintone_preference == 2 ~ "moderately prefer\nDark Skinned People",
      skintone_preference == 3 ~ "slightly prefer\nDark Skinned People",
      skintone_preference == 4 ~ "I like Light Skinned People and\nDark\nSkinned People equally",
      skintone_preference == 5 ~ "slightly prefer\nLight Skinned People",
      skintone_preference == 6 ~ "moderately prefer\nLight Skinned People",
      skintone_preference == 7 ~ "strongly prefer\nLight Skinned People"
    )
  ) |> 
  ggplot() + 
  aes(
    x = skintone_preference
  ) + 
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Score  
skintonedata |> 
  ggplot() + 
  aes(
    x = score
  ) + 
  geom_histogram()
```

```{r}
#Score Distributions 

#politicalid: 1 "strongly conservative" 2 "moderately conservative" 3 "slightly conservative" 4 "neutral" 5 "slightly liberal" 6 "moderately liberal" 7 "strongly liberal"

2
```


```{r}
#LASSO Regression 

predictor_vars <- c("num_tests", "ethnicity", "race", "politicalid", "skintone_preference", "warmth_dark",
                "warmth_light", "age", "edu", "birthSex")
response_var <- as.vector(skintonedata$score)

X <- as.matrix(skintonedata[, predictor_vars])  # Predictor matrix
Y <- response_var  # Response variable

# Perform Lasso regression
lasso_model <- glmnet(X, Y, alpha = 1)  # alpha = 1 for Lasso regression

# Plot the cross-validated mean squared error (CV MSE) vs lambda
plot(lasso_model)

# Select lambda with minimum CV MSE
best_lambda <- cv.glmnet(X, Y, alpha = 1)$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste("Mean Squared Error (MSE):", mse))

# Print the coefficients
print(coef(lasso_model_best))
```

Non-zero coefficients: Predictors with non-zero coefficients in the printed output are selected by the Lasso regression model. These predictors are deemed to have a significant influence on the response variable.

Zero coefficients: Predictors with coefficients set to zero in the printed output are effectively excluded from the model. These predictors are considered to have little or no impact on the response variable according to the Lasso regularization.

Coefficient values: The magnitude of the coefficients indicates the strength of the relationship between each predictor and the response variable. Larger coefficient values suggest stronger relationships, whereas smaller values suggest weaker relationships.

Intercept term: Lasso regression also estimates an intercept term, which represents the expected value of the response variable when all predictor variables are zero.

```{r}
#LASSO Regression 2
predictor_vars <- c('num_tests', 'ethnicity', 'race', 'politicalid', 'skintone_preference', 'warmth_dark',
                'warmth_light', 'age', 'edu', 'birthSex')

X <- as.matrix(skintonedata[, predictor_vars])  # Predictor matrix
Y <- skintonedata$score  # Response variable

# Create a grid of lambda values for cross-validation
lambda_grid <- 10^seq(10, -2, length = 100)

# Perform cross-validated Lasso regression
lasso_model_cv <- cv.glmnet(X, Y, alpha = 1, lambda = lambda_grid, nfolds = 10)

# Plot mean squared error (MSE) vs lambda
plot(lasso_model_cv)

# Select lambda with minimum MSE
best_lambda <- lasso_model_cv$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste('Mean Squared Error (MSE):', mse))

# Print the coefficients
print(coef(lasso_model_best))

print(best_lambda)

```


```{r}
#Dot Plot 
library(reshape2)
df <- data.frame(
  Row = c("Sex", "Education", "Political Identity", "Age", "Race", "Ethnicity", "Number of Tests", 
                  "Warmth 1", "Warmth 2", "Warmth 3"),
  Skintone = c(0, 1, 1, 1, 0, 1, 1, 0, 0, 1),
  Weight = c(1, 0, 1, 1, 0, 0, 1, 1, 1, 1),
  Gender = c(1, 0, 1, 1, 0, 0, 1, 1, 1, 1),
  Sexuality = c(1, 0, 1, 1, 1, 0, 1, 1, 1, 1)
)

data_long <- melt(df, id.vars = "Row", variable.name = "Category", value.name = "Value")
data_long$Row <- factor(data_long$Row, levels = c("Political Identity", "Number of Tests", "Age", "Warmth 3",
                                                  "Warmth 2", "Warmth 1", "Sex", "Race", "Ethnicity", "Education"))

data_long |> 
  ggplot() + 
  aes(x = Category, y = Row, color = as.factor(Value)) + 
  geom_point(size = 5) +
  scale_color_manual(values = c("red", "green"), labels = c("No", "Yes")) +
  labs(title = "LASSO Regression Variable Selection",
       x = "Bias Type",
       y = "Predictors",
       color = "Variable was selected:",
       caption = "Warmth 1, 3, and 3 represent how warm or cold a participant feels towards the groups\nof people in that bias test. See __ for more specifics on the warmth questions for each test.") +
  theme_minimal() +
  theme(legend.position = "bottom") 
```

```{r}
#Multiple linear regression 

predictor_vars <- c("num_tests", "ethnicity", "politicalid", "skintone_preference",
                    "warmth_light", "age", "edu", "birthSex")

# Set seed for reproducibility (allows for the same random split when rerunning)
set.seed(123)

# Generate indices for train and test data
train_indices <- sample(1:nrow(skintonedata), 0.8 * nrow(skintonedata))  # 80% for training
test_indices <- setdiff(1:nrow(skintonedata), train_indices)  # Remaining for testing

# Create training and test data frames
train_data <- skintonedata[train_indices, ]
test_data <- skintonedata[test_indices, ]

train_data <- train_data[, c(predictor_vars, "score")]
test_data <- test_data[, c(predictor_vars, "score")]

#Model
model <- lm(score ~ num_tests + ethnicity + politicalid+skintone_preference+
                    warmth_light+age+edu+birthSex, data = train_data)

# Summary of the model
summary(model)

# Predictions
predictions <- predict(model, test_data)

# Visualize actual vs predicted values
plot(test_data$score, predictions, xlab = "Actual", ylab = "Predicted", main = "Actual vs Predicted")
abline(0, 1, col = "red")  # add a 45-degree line for comparison
```

```{r}
#Best Subset Selection 1

# Perform best subset selection
subset_model <- regsubsets(score ~ num_tests + ethnicity + politicalid + skintone_preference +
                    warmth_light + age + edu + birthSex, data = skintonedata, nvmax = 6)

# Get the best model based on adjusted R-squared
best_model <- which.max(summary(subset_model)$adjr2)

# Summary of the best model
summary(subset_model, id = best_model)
```

```{r}
#Best Subset Selection 2

# Perform best subset selection
subset_model <- regsubsets(score ~ num_tests + ethnicity + politicalid + skintone_preference +
                    warmth_light + age + edu + birthSex, data = skintonedata, nvmax = 4)

# Get the best model based on adjusted R-squared
best_model <- which.max(summary(subset_model)$adjr2)

# Summary of the best model
summary(subset_model, id = best_model)
```

```{r}
#Best Subset Selection 3

# Perform best subset selection
subset_model <- regsubsets(score ~ num_tests + ethnicity + politicalid + skintone_preference +
                    warmth_light + age + edu + birthSex, data = skintonedata, nvmax = 3)

# Get the best model based on adjusted R-squared
best_model <- which.max(summary(subset_model)$adjr2)

# Summary of the best model
summary(subset_model, id = best_model)
```

```{r}
#Best Subset Selection 4

# Perform best subset selection
subset_model <- regsubsets(score ~ num_tests + ethnicity + politicalid + skintone_preference +
                    warmth_light + age + edu + birthSex, data = skintonedata, nvmax = 8)

# Get the best model based on adjusted R-squared
best_model <- which.max(summary(subset_model)$adjr2)

# Summary of the best model
summary(subset_model, id = best_model)
```

```{r}
#Logistic Regression - create categories

#create categories in score variable (no bias, moderate bias, strong bias)

# Define the breaks for creating three categories
breaks <- c(-Inf, -0.0001, 0.0001, 0.33, 0.66, Inf)

# Create a new categorical variable based on the breaks
skintonedata$scorecat <- cut(skintonedata$score, breaks = breaks, labels = c("Opposite", "None", "Low", "Medium", "High"))

# Print the summary of the new categorical variable
summary(skintonedata$scorecat)
```

```{r}
#Training and Test Data 

# Set seed for reproducibility (allows for the same random split when rerunning)
set.seed(123)

# Generate indices for train and test data
train_indices <- sample(1:nrow(skintonedata), 0.8 * nrow(skintonedata))  # 80% for training
test_indices <- setdiff(1:nrow(skintonedata), train_indices)  # Remaining for testing

# Create training and test data frames
train_data <- skintonedata[train_indices, ]
test_data <- skintonedata[test_indices, ]

#train_data <- train_data[, c(predictor_vars, "scorecat")]
#test_data <- test_data[, c(predictor_vars, "scorecat")]
```

```{r}
#Logistic regression (og)

predictor_vars0 <- c("num_tests", "ethnicity", "politicalid", "skintone_preference",
                    "warmth_light", "age", "edu", "birthSex")

train_data0 <- train_data[, c(predictor_vars0, "scorecat")]
test_data0 <- test_data[, c(predictor_vars0, "scorecat")]

# Fit multinomial logistic regression model
multinom_model0 <- multinom(scorecat ~ ., data = train_data0)

# Print summary of the model
summary(multinom_model0)
```

```{r}
#Logistic Regression bestsubset regression 1

predictor_vars1 <- c("num_tests", "ethnicity", "politicalid", "skintone_preference",
                    "age", "edu")

train_data1 <- train_data[, c(predictor_vars1, "scorecat")]
test_data1 <- test_data[, c(predictor_vars1, "scorecat")]

# Fit multinomial logistic regression model
multinom_model1 <- multinom(scorecat ~ ., data = train_data1)

# Print summary of the model
summary(multinom_model1)
```

```{r}
#Logistic Regression bestsubset regression 2

predictor_vars2 <- c("num_tests", "politicalid", "skintone_preference", "age")

train_data2 <- train_data[, c(predictor_vars2, "scorecat")]
test_data2 <- test_data[, c(predictor_vars2, "scorecat")]

# Fit multinomial logistic regression model
multinom_model2 <- multinom(scorecat ~ ., data = train_data2)

# Print summary of the model
summary(multinom_model2)
```

```{r}
#Logistic Regression best subset regression 3

predictor_vars3 <- c("num_tests", "skintone_preference", "age")

train_data3 <- train_data[, c(predictor_vars3, "scorecat")]
test_data3 <- test_data[, c(predictor_vars3, "scorecat")]

# Fit multinomial logistic regression model
multinom_model3 <- multinom(scorecat ~ ., data = train_data3)

# Print summary of the model
summary(multinom_model3)
```

```{r}
#Logistic Regression best subset regression 4

predictor_vars4 <- c("num_tests", "ethnicity", "politicalid", "skintone_preference",
                    "warmth_light", "age", "edu", "birthSex")

train_data4 <- train_data[, c(predictor_vars4, "scorecat")]
test_data4 <- test_data[, c(predictor_vars4, "scorecat")]

# Fit multinomial logistic regression model
multinom_model4 <- multinom(scorecat ~ ., data = train_data4)

# Print summary of the model
summary(multinom_model4)
```


```{r}
#Logistic Regression Visual 

# Predict classes
predicted_classes <- predict(multinom_model, newdata = test_data, type = "class")

# Create confusion matrix
conf_matrix <- table(test_data$scorecat, predicted_classes)

# Print confusion matrix
print(conf_matrix)
```

```{r}
# Predict probabilities for each class
predicted_probs <- predict(multinom_model, newdata = skintonedata, type = "probs")

# Extract the predicted probabilities for the true class
true_class_probs <- predicted_probs[cbind(1:nrow(skintonedata), as.numeric(skintonedata$scorecat))]

# Calculate log loss for each observation
log_loss <- -log(true_class_probs)

# Plot the distribution of log loss values
hist(log_loss, main = "Distribution of Log Loss", 
     xlab = "Log Loss", 
     ylab = "Frequency", 
     col = "skyblue", 
     border = "white")
```

This visualization provides insights into the distribution of log loss values, which is a measure of the accuracy of the predicted probabilities. Lower log loss values indicate better calibration of predicted probabilities to the true outcomes.

The log loss is defined as the negative natural logarithm of the predicted probability assigned to the true class. This value is commonly used as a measure of the accuracy of predicted probabilities in classification tasks.

```{r, fig.width=11, fig.height=6}
#Regression Tree 

predictor_vars <- c("num_tests", "ethnicity", "race", "politicalid", "skintone_preference", "warmth_dark",
                "warmth_light")
response_var <- skintonedata$score

# Fit regression tree model
#tree_model <- rpart(response_var ~ ., data = skintonedata[, c(predictor_vars, "scorecat")], method = "anova")
tree_model <- rpart(response_var ~ ., data = skintonedata[, c(predictor_vars, "scorecat")], method = "class")

# Print the tree
print(tree_model)

# Plot the tree
plot(tree_model)
text(tree_model)
```


### Gender data 

```{r}
genderdata <- read_sav("/Users/gracelock/Downloads/Gender-Career IAT.public.2023.sav")
```

```{r}
genderdata |> select("session_id", "birthyear", "num_002", "birthSex", "ethnicityomb", "edu",
                     "raceomb_002", "D_biep.Male_Career_all", "impcareer", "impfamily", "politicalid_7") |> 
              na.omit("D_biep.Male_Career_all") |>
              mutate(age = 2024-birthyear) |> 
              select(-birthyear) |> 
              rename("num_tests" = "num_002",
                     "ethnicity" = "ethnicityomb",
                     "race" = "raceomb_002",
                     "politicalid" = "politicalid_7",
                     "score" = "D_biep.Male_Career_all",
                     "att_family" = "impfamily",
                     "att_carerr" = "impcareer") -> genderdata
```

```{r}
#LASSO Regression 

predictor_vars <- c("num_tests", "ethnicity", "race", "politicalid", "att_family", "att_carerr",
                     "age", "edu", "birthSex")
response_var <- as.vector(genderdata$score)

X <- as.matrix(genderdata[, predictor_vars])  # Predictor matrix
Y <- response_var  # Response variable

# Perform Lasso regression
lasso_model <- glmnet(X, Y, alpha = 1)  # alpha = 1 for Lasso regression

# Plot the cross-validated mean squared error (CV MSE) vs lambda
plot(lasso_model)

# Select lambda with minimum CV MSE
best_lambda <- cv.glmnet(X, Y, alpha = 1)$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste("Mean Squared Error (MSE):", mse))

# Print the coefficients
print(coef(lasso_model_best))
```

```{r}
#LASSO Regression 2
#predictor_vars <- c('num_tests', 'ethnicity', 'race', 'politicalid', 'skintone_preference', 'warmth_dark',
                #'warmth_light', 'age', 'edu', 'birthSex')

X <- as.matrix(genderdata[, predictor_vars])  # Predictor matrix
Y <- genderdata$score  # Response variable

# Create a grid of lambda values for cross-validation
lambda_grid <- 10^seq(10, -2, length = 100)

# Perform cross-validated Lasso regression
lasso_model_cv <- cv.glmnet(X, Y, alpha = 1, lambda = lambda_grid, nfolds = 10)

# Plot mean squared error (MSE) vs lambda
plot(lasso_model_cv)

# Select lambda with minimum MSE
best_lambda <- lasso_model_cv$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste('Mean Squared Error (MSE):', mse))

# Print the coefficients
print(coef(lasso_model_best))

print(best_lambda)
```


### Sexuality data

```{r}
sexualitydata <- read_sav("/Users/gracelock/Downloads/Sexuality IAT.public.2023.sav")
```

```{r}
sexualitydata |> select("session_id", "birthyear", "num_002", "birthSex", "ethnicityomb", "edu",
                     "raceomb_002", "D_biep.Straight_Good_all", "Tgayleswomen", "Tgaymen", "Tstraightmen",
                     "politicalid_7") |>
                na.omit("D_biep.Straight_Good_all") |>
                mutate(age = 2024-birthyear) |> 
                select(-birthyear) |> 
                rename("num_tests" = "num_002",
                      "ethnicity" = "ethnicityomb",
                      "race" = "raceomb_002",
                      "politicalid" = "politicalid_7",
                      "score" = "D_biep.Straight_Good_all",
                      "warmth_gayleswomen" = "Tgayleswomen",
                      "warmth_gaymen" = "Tgaymen",
                      "warmth_straightmen" = "Tstraightmen") -> sexualitydata
```

```{r}
#LASSO Regression 

predictor_vars <- c("num_tests", "ethnicity", "race", "politicalid", "warmth_gayleswomen", "warmth_gaymen",
                     "warmth_straightmen", "age", "edu", "birthSex")
response_var <- as.vector(sexualitydata$score)

X <- as.matrix(sexualitydata[, predictor_vars])  # Predictor matrix
Y <- response_var  # Response variable

# Perform Lasso regression
lasso_model <- glmnet(X, Y, alpha = 1)  # alpha = 1 for Lasso regression

# Plot the cross-validated mean squared error (CV MSE) vs lambda
plot(lasso_model)

# Select lambda with minimum CV MSE
best_lambda <- cv.glmnet(X, Y, alpha = 1)$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste("Mean Squared Error (MSE):", mse))

# Print the coefficients
print(coef(lasso_model_best))
```

```{r}
#LASSO Regression 2
#predictor_vars <- c('num_tests', 'ethnicity', 'race', 'politicalid', 'skintone_preference', 'warmth_dark',
                #'warmth_light', 'age', 'edu', 'birthSex')

X <- as.matrix(sexualitydata[, predictor_vars])  # Predictor matrix
Y <- sexualitydata$score  # Response variable

# Create a grid of lambda values for cross-validation
lambda_grid <- 10^seq(10, -2, length = 100)

# Perform cross-validated Lasso regression
lasso_model_cv <- cv.glmnet(X, Y, alpha = 1, lambda = lambda_grid, nfolds = 10)

# Plot mean squared error (MSE) vs lambda
plot(lasso_model_cv)

# Select lambda with minimum MSE
best_lambda <- lasso_model_cv$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste('Mean Squared Error (MSE):', mse))

# Print the coefficients
print(coef(lasso_model_best))

print(best_lambda)
```


### Weight data 

```{r}
weightdata <- read_sav("/Users/gracelock/Downloads/Weight IAT.public.2023.sav")
```

```{r}
weightdata |> select("session_id", "birthyear", "num_002", "birthSex", "ethnicityomb", "edu", "politicalid_7",
                     "raceomb_002", "D_biep.Thin_Good_all", "att7", "tthin", "tfat", "comptomost_001") |>
              na.omit("D_biep.Thin_Good_all") |>
              mutate(age = 2024-birthyear) |> 
              select(-birthyear) |> 
              rename("num_tests" = "num_002",
                      "ethnicity" = "ethnicityomb",
                      "race" = "raceomb_002",
                      "score" = "D_biep.Thin_Good_all",
                      "body_preference" = "att7",
                      "warmth_thin" = "tthin",
                      "warmth_fat" = "tfat",
                      "comptomost" = "comptomost_001",
                      "politicalid" = "politicalid_7") -> weightdata

#comptomost: 1 "Much thinner" 2 "Moderately thinner" 3 "Slightly thinner" 4 "About the same" 5 "Slightly heavier" 6 "Moderately heavier" 7 "Much heavier"
```

```{r}
#LASSO Regression 

predictor_vars <- c("num_tests", "ethnicity", "race", "politicalid", "warmth_thin", "warmth_fat",
                     "comptomost", "age", "edu", "birthSex")
response_var <- as.vector(weightdata$score)

X <- as.matrix(weightdata[, predictor_vars])  # Predictor matrix
Y <- response_var  # Response variable

# Perform Lasso regression
lasso_model <- glmnet(X, Y, alpha = 1)  # alpha = 1 for Lasso regression

# Plot the cross-validated mean squared error (CV MSE) vs lambda
plot(lasso_model)

# Select lambda with minimum CV MSE
best_lambda <- cv.glmnet(X, Y, alpha = 1)$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste("Mean Squared Error (MSE):", mse))

# Print the coefficients
print(coef(lasso_model_best))
```

```{r}
#LASSO Regression 2
#predictor_vars <- c('num_tests', 'ethnicity', 'race', 'politicalid', 'skintone_preference', 'warmth_dark',
                #'warmth_light', 'age', 'edu', 'birthSex')

X <- as.matrix(weightdata[, predictor_vars])  # Predictor matrix
Y <- weightdata$score  # Response variable

# Create a grid of lambda values for cross-validation
lambda_grid <- 10^seq(10, -2, length = 100)

# Perform cross-validated Lasso regression
lasso_model_cv <- cv.glmnet(X, Y, alpha = 1, lambda = lambda_grid, nfolds = 10)

# Plot mean squared error (MSE) vs lambda
plot(lasso_model_cv)

# Select lambda with minimum MSE
best_lambda <- lasso_model_cv$lambda.min

# Refit the model with the selected lambda
lasso_model_best <- glmnet(X, Y, alpha = 1, lambda = best_lambda)

# Make predictions
predictions <- predict(lasso_model_best, newx = X)

# Calculate MSE
mse <- mean((predictions - Y)^2)

# Print MSE
print(paste('Mean Squared Error (MSE):', mse))

# Print the coefficients
print(coef(lasso_model_best))

print(best_lambda)
```


### Citations

```{r}
rpart_cit <- citation("rpart")
glmnet_cit <- citation("glmnet")
```


